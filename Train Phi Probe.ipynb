{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08198559-0546-4a9c-b616-9ce2842fd1f7",
   "metadata": {},
   "source": [
    "## Load Common Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecefaac6-9f6f-4588-b8df-6b2f4959fb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.40.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0.post304)\n",
      "Requirement already satisfied: torchinfo in /opt/conda/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.10)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-08-03 16:21:54.282196: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-03 16:21:54.319653: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84df3d4460d84a0abae702ceb44f3901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "%run Common\\ Code.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b499c76c-2591-453a-a9bc-e8b89c2d65ea",
   "metadata": {},
   "source": [
    "## Now define our autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f93598-167b-449e-9573-90e038d86cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, activation_dimension=2560, inner_dimension=100_000):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(activation_dimension, inner_dimension),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.decoder = torch.nn.Linear(inner_dimension, activation_dimension)\n",
    "\n",
    "    def forward(self, activations):\n",
    "        encoded = self.encoder(activations)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded  # Return encoded too, as it's used in the loss fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871ec152-cd46-43f4-b4f2-a7e0074439ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path('sae.pt').exists():\n",
    "    sae = torch.load('sae.pt').to(device)\n",
    "    is_trained = True\n",
    "else:\n",
    "    sae = SparseAutoencoder()\n",
    "    is_trained = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "395fa780-5d92-4742-8978-9fcb9d77430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "def calculate_loss(sae_model, prediction, target, feature_activations, lamb=5):\n",
    "    weight_norms = torch.norm(sae_model.decoder.weight, dim=0, p=2).view(-1)\n",
    "    feature_sizes = torch.abs(feature_activations)\n",
    "    sparsity_loss = lamb * torch.sum(weight_norms * feature_sizes)\n",
    "\n",
    "    prediction_loss = F.mse_loss(prediction, target, reduction='sum')\n",
    "\n",
    "    total_loss = prediction_loss + sparsity_loss\n",
    "    \n",
    "    return total_loss, prediction_loss, sparsity_loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d944680f-4a0f-470b-b2a3-798cc0f3e071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15655829504 7822376960 7616183296 206193664\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t, r, a, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb24c23c-418a-4b12-acdf-bb8117887a64",
   "metadata": {},
   "source": [
    "## Train our autoencoder on 1M tokens' worth of activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048f3ee-14c2-490b-b308-348f92ca2380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokens seen: 25000; Loss: 21140.35; Sparsity-specific loss: 10790.55:   2%|▏         | 49/2000 [00:57<38:25,  1.18s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen 25000 tokens\n",
      "\t - Training data: loss is 21140.35; sparsity loss is 10790.55\n",
      "\t - Eval data: loss is 10860.99; sparsity loss is 7113.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokens seen: 30000; Loss: 19272.12; Sparsity-specific loss: 10045.19:   3%|▎         | 60/2000 [01:18<42:05,  1.30s/it]"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_eval_loss(sae_model, minibatches, tokens_per_minibatch):\n",
    "    losses = []\n",
    "    sparsities = []\n",
    "    accuracies = []\n",
    "    for tokens, activations in minibatches:\n",
    "        decoded, encoded = sae_model(activations)\n",
    "        loss, accuracy, sparsity = calculate_loss(sae_model, decoded, activations, encoded)\n",
    "        losses.append(loss.item() / tokens_per_minibatch)\n",
    "        accuracies.append(accuracy.item() / tokens_per_minibatch)        \n",
    "        sparsities.append(sparsity.item() / tokens_per_minibatch)        \n",
    "    return sum(losses) / len(losses), sum(accuracies) / len(accuracies), sum(sparsities) / len(sparsities)\n",
    "\n",
    "def train(sae_model, num_tokens=1000000, minibatch_size=5, block_size=100, loss_history=100, adam_beta1=0.9, adam_beta2=0.999, lr=5E-5, eval_tokens=1000, show_eval_token_interval=25000, start_block=0):            \n",
    "    optimizer = torch.optim.AdamW(sae_model.parameters(), lr=lr, betas=(adam_beta1, adam_beta2))\n",
    "    tokens_per_minibatch = minibatch_size * block_size\n",
    "    num_minibatches = num_tokens // tokens_per_minibatch\n",
    "    eval_minibatches = eval_tokens // tokens_per_minibatch\n",
    "    losses = []\n",
    "    sparsities = []\n",
    "    activations_gen = PhiProbeCommons.all_activations('train', minibatch_size=minibatch_size, block_size=block_size + start_block)\n",
    "    test_activations_gen = PhiProbeCommons.all_activations('test', minibatch_size=minibatch_size, block_size=block_size, start_block=1000+start_block)    \n",
    "    \n",
    "    training_data = islice(activations_gen, num_minibatches)         \n",
    "    show_eval_minibatch_interval = show_eval_token_interval // tokens_per_minibatch\n",
    "\n",
    "    pbar = tqdm(training_data, total=num_minibatches, smoothing=0)\n",
    "    for idx, (tokens, activations) in enumerate(pbar):        \n",
    "        decoded, encoded = sae_model(activations)        \n",
    "        loss, accuracy, sparsity = calculate_loss(sae_model, decoded, activations, encoded)        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        losses.append(loss.item() / tokens_per_minibatch)\n",
    "        sparsities.append(sparsity.item() / tokens_per_minibatch)\n",
    "        if len(losses) > loss_history:\n",
    "            losses = losses[1:]\n",
    "            sparsities = sparsities[1:]\n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        avg_sparsity = sum(sparsities) / len(sparsities)\n",
    "        tokens_seen = (idx + 1) * tokens_per_minibatch\n",
    "        pbar.set_description(f'Tokens seen: {tokens_seen}; Loss: {avg_loss:.2f}; Sparsity-specific loss: {avg_sparsity:.2f}')\n",
    "        if (idx + 1) % show_eval_minibatch_interval == 0:\n",
    "            minibatches = islice(test_activations_gen, eval_minibatches)\n",
    "            eval_loss, _, eval_sparsity = get_eval_loss(sae_model, minibatches, tokens_per_minibatch)\n",
    "            print(f'Seen {tokens_seen} tokens')\n",
    "            print(f'\\t - Training data: loss is {avg_loss:.2f}; sparsity loss is {avg_sparsity:.2f}')\n",
    "            print(f'\\t - Eval data: loss is {eval_loss:.2f}; sparsity loss is {eval_sparsity:.2f}')            \n",
    "            torch.save(sae, 'sae.pt')\n",
    "\n",
    "if not is_trained:\n",
    "    train(sae)\n",
    "    is_trained = True\n",
    "    torch.save(sae, 'sae.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be28aa-69c9-4e73-9187-a2990a7e6170",
   "metadata": {},
   "source": [
    "## Now let's see how our accuracy and sparsity do compared to a random Autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42a210-f31a-42b3-aa3a-134c2a692c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae2 = SparseAutoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a23cb8-f90d-4e6e-81e0-a6e8a376d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, activations = next(all_activations('test', minibatch_size=5, block_size=100, start_block=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af6540b-cba3-4c29-bdf4-4500f9f9f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(f'Testing on minibatch including:\\n---\\n{tokenizer.decode(tokens[0])}\\n---\\n')\n",
    "    trained_decoded, trained_encoded = sae(activations)    \n",
    "    untrained_decoded, untrained_encoded  = sae2(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced93995-b4c5-4497-99f1-a0701a67c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    trained_total, trained_accuracy, trained_sparsity = calculate_loss(sae, trained_decoded, activations, trained_encoded)\n",
    "    untrained_total, untrained_accuracy, untrained_sparsity = calculate_loss(sae2, untrained_decoded, activations, trained_encoded)\n",
    "\n",
    "print('Untrained model:')\n",
    "print(f'\\t- Accuracy loss: {untrained_accuracy / 2500:.1f}')\n",
    "print(f'\\t- Sparsity loss: {untrained_sparsity / 2500:.1f}')\n",
    "print(f'\\t- Total loss: {untrained_total / 2500:.1f}')\n",
    "print('')\n",
    "print('Trained model:')\n",
    "print(f'\\t- Accuracy loss: {trained_accuracy / 2500:.1f}')\n",
    "print(f'\\t- Sparsity loss: {trained_sparsity / 2500:.1f}')\n",
    "print(f'\\t- Total loss: {trained_total / 2500:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80446f8b-bd81-4ddf-9022-2af9a09ef415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
