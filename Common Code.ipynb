{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc67cc4a-347b-45cd-bdb1-c3bc641c35ed",
   "metadata": {},
   "source": [
    "## Set up dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f917e4-5b5b-4e0e-a1bb-aa7df0ad62e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.40.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0.post304)\n",
      "Requirement already satisfied: torchinfo in /opt/conda/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.10)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch torchinfo tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "967f09f2-a99a-45e1-b6f2-ff8eb3fd596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datasets import load_dataset, Dataset, IterableDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e779b-4448-4e99-a3db-1fb0752666c1",
   "metadata": {},
   "source": [
    "## Set up a class to serve as a namespace for our exports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117c9cd7-f6ab-42ee-a512-1fdcc42b82fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiProbeCommons:\n",
    "    # We'll fill this in with utilities as they're defined below.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f279a0-f815-4076-98c0-932e9d761a3e",
   "metadata": {},
   "source": [
    "## Download the Dolma v1.6_sample dataset (~16GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a258afee-5fde-48d5-b5d8-11ea42721528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _data():\n",
    "    return load_dataset('allenai/dolma', name='v1_6-sample', trust_remote_code=True, streaming=True, split=\"train\")\n",
    "\n",
    "PhiProbeCommons.data = _data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3507ae68-83fe-4afa-8d93-6cf223a8a42c",
   "metadata": {},
   "source": [
    "## Load phi-2 and set up a method that gets the middle layer's activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "568eb6c5-8303-4d55-818e-b2bbc481e74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-07-25 17:36:31.821871: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-25 17:36:31.859638: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6f05bb0ee24f5381232d0aaf8eac7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "\n",
    "PhiProbeCommons.phi = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=torch.float16, trust_remote_code=True)\n",
    "PhiProbeCommons.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "\n",
    "# Suppress warnings about tokenising long passages. \n",
    "# We're going to handle splitting the text ourselves.\n",
    "PhiProbeCommons.tokenizer.model_max_length = sys.maxsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "303c6350-0a75-4628-a72f-6a33eb9ca754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize(text):\n",
    "    return PhiProbeCommons.tokenizer(text, return_tensors=\"pt\", return_attention_mask=False)['input_ids']\n",
    "\n",
    "PhiProbeCommons.tokenize = _tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "951eb3c6-fe26-4502-aae6-8045718229ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def _get_activations(tokens):\n",
    "    \"\"\"\n",
    "    Takes a tensor of shape e.g. (B, T) containing a batch of token strings,\n",
    "    and outputs a tensor of shape (B, T, 2560) containing the activations\n",
    "    of the middle layer (hidden layer 15) at each token.\n",
    "    \"\"\"\n",
    "    out = PhiProbeCommons.phi(tokens, output_hidden_states=True, return_dict=True)\n",
    "    hidden_states = out.hidden_states\n",
    "    middle_layer = hidden_states[16]\n",
    "    return middle_layer.float()\n",
    "\n",
    "PhiProbeCommons.get_activations = _get_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2666b9b6-7dcd-4aac-9cee-9d7d860e4d96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _get_token_blocks(text, block_size):\n",
    "    \"\"\"\n",
    "    Given a text, tokenizes it and returns a tensor of shape \n",
    "    (N, BS) containing the text as a series of blocks of \n",
    "    tokens of size BS.\n",
    "    N is the number of blocks contained within the text.   \n",
    "    \"\"\"    \n",
    "    block_size = 100\n",
    "    tokens = PhiProbeCommons.tokenize(text).view(-1)\n",
    "    truncated_length = (len(tokens) // block_size) * block_size\n",
    "    tokens = tokens[:truncated_length]\n",
    "    block_tensor = tokens.view(-1, block_size)\n",
    "    blocks = torch.split(block_tensor, 1, dim=0)    \n",
    "    return [b.view(-1) for b in blocks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e89575e7-8714-4819-af4d-606cd03a48f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _all_blocks(split, block_size=100):    \n",
    "    for i, example in enumerate(PhiProbeCommons.data()):\n",
    "        is_test_example = i % 10 == 0\n",
    "        if split == 'train' and is_test_example or split != 'train' and not is_test_example:\n",
    "            continue\n",
    "        for block in _get_token_blocks(example['text'], block_size=block_size):                        \n",
    "            yield block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24182c0b-6359-4856-85f8-8867d67c30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_iterable(iterable, batch_size):\n",
    "    current_batch = []\n",
    "    for item in iterable:\n",
    "        current_batch.append(item)\n",
    "        if len(current_batch) == batch_size:\n",
    "            yield current_batch\n",
    "            current_batch = []\n",
    "\n",
    "def _all_activations(split, block_size=100, minibatch_size=5, start_block=0, parallelism=10, workahead_cache_size=50):\n",
    "    \"\"\"\n",
    "    Generator yielding minibatches of training examples.\n",
    "    Each example is returned as a tuple (tokens, activations).\n",
    "     - tokens is a tensor of dimention (MB, BS) where MB \n",
    "       is the specified minibatch size, and BS is the block size.\n",
    "     - activations is a tensor of dimension (MB, BS, 2560)\n",
    "    \"\"\"    \n",
    "    global phi\n",
    "    assert workahead_cache_size % parallelism == 0\n",
    "    assert workahead_cache_size % minibatch_size == 0\n",
    "    assert parallelism % minibatch_size == 0\n",
    "    blocks = islice(_all_blocks(split, block_size=block_size), start_block, None)\n",
    "    for block_megabatch in _batch_iterable(blocks, workahead_cache_size):  \n",
    "        PhiProbeCommons.phi = PhiProbeCommons.phi.to(\"cuda\")\n",
    "        minibatches = []        \n",
    "        for inputs in torch.stack(block_megabatch).split(parallelism):            \n",
    "            activations = PhiProbeCommons.get_activations(inputs)      \n",
    "            minibatch_inputs = inputs.split(minibatch_size)\n",
    "            minibatch_activations = activations.split(minibatch_size)\n",
    "            minibatches.extend(zip(minibatch_inputs, minibatch_activations))\n",
    "        PhiProbeCommons.phi = PhiProbeCommons.phi.to(\"cpu\")\n",
    "        \n",
    "        for minibatch_input, minibatch_activation in minibatches:\n",
    "            yield minibatch_input, minibatch_activation\n",
    "\n",
    "PhiProbeCommons.all_activations = _all_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b499c76c-2591-453a-a9bc-e8b89c2d65ea",
   "metadata": {},
   "source": [
    "## Load Our Pretrained Autoencoder (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "871ec152-cd46-43f4-b4f2-a7e0074439ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "PhiProbeCommons._sae = None\n",
    "\n",
    "def _load_sae():\n",
    "    if PhiProbeCommons._sae is None and Path('sae.pt').exists():\n",
    "        PhiProbeCommons._sae = torch.load('sae.pt').to(device)\n",
    "    return PhiProbeCommons._sae\n",
    "\n",
    "PhiProbeCommons.sae = _load_sae"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
